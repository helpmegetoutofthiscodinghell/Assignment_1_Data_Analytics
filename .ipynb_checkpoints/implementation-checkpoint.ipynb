{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a53d19-7d16-41fb-8528-2570f427f6ac",
   "metadata": {},
   "source": [
    "# Balanced Risk Set Matching: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5021b-daa9-43e8-9310-751fc72df18d",
   "metadata": {},
   "source": [
    "## Notations Used\n",
    "\n",
    "| Notation/Variable | Description | Section |\n",
    "| --- | --- | ---|\n",
    "| $T_m$ | Time when patient $m$ received treatment. If $T_m$ = $\\inf$, the patient was not treated. | 1.2 (Risk Set Matching) |\n",
    "| $Y_i(t)$ | Observed symptoms (e.g., pain, urgency) of patient $i$ at time $t$ | 1.2 (Risk Set Matching) |\n",
    "| SymptomHistory(T) | Aggregated symptom history up to time $T$. | 1.2 (Risk Set Matching) |\n",
    "| $\\mathcal{A}$ | Set of all patients included in the study | 2.1 (Optimal Balanced Matching) |\n",
    "| $\\mathcal{T}$ | Subset of treated patients in $\\mathcal{A}$ | 2.1 (Optimal Balanced Matching) | \n",
    "| $\\mathcal{A}$ - $\\mathcal{T}$ | Subset of controls in $\\mathcal{A}$ | 2.1 (Optimal Balanced Matching) |\n",
    "| $\\mathcal{E}$ | Set of all possible edges $\\mathcal{e}$ |  2.1 (Optimal Balanced Matching) |\n",
    "| $\\mathcal{e}$ = ($\\alpha_p$, $\\alpha_q$) | An edge connecting a treated patient $\\alpha_p$ with a potential control $\\alpha_q$ | 2.1 (Optimal Balanced Matching) |\n",
    "| $\\delta_e$ | Distance between two patients connected by edge $\\mathcal{e}$, often measured using Mahalanobis distance. | 2.1 (Optimal Balanced Matching) |\n",
    "| $S$ | Number of matched pairs. | 2.1 (Optimal Balanced Matching) |\n",
    "| $M$ | Set of matched pairs $S\\subset\\mathcal{E}$ | 2.1 (Optimal Balanced Matching) |\n",
    "| $B_{pk},B_{ek}$ | Binary variables representing attributes (e.g., symptoms or covariates) for treated/control patients. | 2.2 (Balanced Pair Matching) |\n",
    "| $g_k^+,g_k^-$ | Gap variables indicating positive or negative imbalance in binary variables. | Appendix |\n",
    "| $\\mathcal{L}_t(a)$ | Risk set: Patients with observed covariates $a$ at time $t$. | 4.3 (Matching on Observed Histories |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08eaafb-e90e-46da-bf92-a728946eadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np # For Data Manipulation\n",
    "import pandas as pd # For Data visualization\n",
    "\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from ortools.graph.python import min_cost_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b451e-4165-4317-a9c7-71766f8d7b57",
   "metadata": {},
   "source": [
    "## I. Interstitial Cystitis Data Set\n",
    "\n",
    "The data, or the patients, in the study is represented in a vector of covariates (i.e. a variable that is observed or measured in a study, which may influence the outcome)\n",
    "\n",
    "These covariates include (There were SIX Covariates in the study):\n",
    "- Baseline Values:\n",
    "  - Pain at baseline ($P_{baseline}$)\n",
    "  - Urgency at baseline ($U_{baseline}$)\n",
    "  - Nocturnal Frequency of voiding i.e. peeing at night ($F_{baseline}$}\n",
    "- Values at treatment time ($T_p$):\n",
    "  - Pain at the time of treatment ($P_{T_p}$)\n",
    "  - Urgency at the time of treatment ($U_{T_p}$)\n",
    "  - Nocturnal Frequency of voiding at the time of treatment i.e. peeing at night ($F_{T_p}$)\n",
    "\n",
    "These are time-dependent covariates because their values evolve over time.\n",
    "\n",
    "Thus, this represents a patient vector with the six covariates:\n",
    "\n",
    "\\begin{equation}\n",
    "a_p = (P_{baseline}, U_{baseline}, F_{baseline}, P_{T_p}, U_{T_p}, F_{T_p})\n",
    "\\end{equation}\n",
    "\n",
    "NOTE: I also added more variables like an indicator if the patient is treated (Treatment Status) and if so, how long has he/she/they been treated (Treatment Time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14760c-d1b8-45ef-bfef-640f13a084c7",
   "metadata": {},
   "source": [
    "### Loading the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7e9481-5b73-426c-9977-fa2ad6c8e665",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'integer_synthetic_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sample Interstitial Data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: Loading the Data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minteger_synthetic_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum (NULL): \u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset (No. of Patients, No. of Variables): \u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'integer_synthetic_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Sample Interstitial Data\n",
    "\n",
    "# Step 1: Loading the Data\n",
    "df = pd.read_csv('integer_synthetic_dataset.csv')\n",
    "print(\"Sum (NULL): \", df.isnull().sum())\n",
    "print(\"Dataset (No. of Patients, No. of Variables): \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c67be-1dce-4132-b973-dc1e2e4baf4e",
   "metadata": {},
   "source": [
    "### Cleaning the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b69fff-d083-4d6f-a4a9-be90750345f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling in the Empty Values\n",
    "df['Treatment Pain'] = df['Treatment Pain'].fillna(-1).astype(int)\n",
    "df['Treatment Urgency'] = df['Treatment Urgency'].fillna(-1).astype(int)\n",
    "df['Treatment Frequency'] = df['Treatment Frequency'].fillna(-1).astype(int)\n",
    "df['Treatment Time'] = df['Treatment Time'].fillna(-1).astype(int)\n",
    "df['Treatment Pain (3 mos)'] = df['Treatment Pain (3 mos)'].fillna(-1).astype(int)\n",
    "df['Treatment Urgency (3 mos)'] = df['Treatment Urgency (3 mos)'].fillna(-1).astype(int)\n",
    "df['Treatment Frequency (3 mos)'] = df['Treatment Frequency (3 mos)'].fillna(-1).astype(int)\n",
    "df['Treatment Pain (6 mos)'] = df['Treatment Pain (6 mos)'].fillna(-1).astype(int)\n",
    "df['Treatment Urgency (6 mos)'] = df['Treatment Urgency (6 mos)'].fillna(-1).astype(int)\n",
    "df['Treatment Frequency (6 mos)'] = df['Treatment Frequency (6 mos)'].fillna(-1).astype(int)\n",
    "print(\"Sum (NULL): \", df.isnull().sum())\n",
    "display(df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00404b8a-d746-4246-bac1-757b89232247",
   "metadata": {},
   "source": [
    "# II. Performing Risk Set Matching\n",
    "\n",
    "Let's go over to the terms described in the section, 1.2 Risk Set Matching:\n",
    "- **Risk Set**: The risk set consists of all patients who are at risk of receiving treatment at a given time $T_m$. When a patient $m$ receives treatment, they are matched with another patient from this risk set who has similar symptom profiles but has not yet received treatment by that time.\n",
    "- **Risk Set Matching**: It is designed to compare a treated patient with a control patient who has not yet received treatment but has as similar history of symptoms up to the time of treatment. This approach aims to create comparable groups by ensuring that the distributions of symptoms are balanced.\n",
    "\n",
    "Mathematical Representation of the Risk Set:\n",
    "\n",
    "\\begin{equation}\n",
    "R(T_m) = \\{j : T_j > T_m \\text{ or } T_j = ∞\\}\n",
    "\\end{equation}\n",
    "\n",
    "Note: We are getting the patients that were treated earlier than the current patient and also the ones that are yet to be treated\n",
    "\n",
    "In summary, risk sets are collections of untreated patients eligible for matching against treated patients based on their symptom histories, rather than being pairs themselves. The aim is to achieve balanced and comparable groups for analysis in observational studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4bbff-aec5-4314-a43d-0a51d595267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct risk sets\n",
    "def construct_risk_set(df):\n",
    "    risk_sets = {}\n",
    "\n",
    "    for index, treated in df[df['Treatment Status'] == 1].iterrows():\n",
    "        T_treated = treated['Treatment Time']\n",
    "\n",
    "        # Select patients who were either never treated (NaN) or treated after T_treated\n",
    "        eligible_controls = df[(df['Treatment Time'].isna()) | \n",
    "                               (df['Treatment Time'] > T_treated)]\n",
    "        \n",
    "        # Store the risk set\n",
    "        risk_sets[treated['Patient ID']] = eligible_controls[['Patient ID', 'Baseline Pain', 'Baseline Urgency', 'Baseline Frequency']]\n",
    "\n",
    "    return risk_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98b146-0cae-4c55-8191-b095a8437e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing Risk Set\n",
    "risk_sets = construct_risk_set(df)\n",
    "\n",
    "# Example output: Print risk set for the first treated patient\n",
    "for index, (patient, controls) in enumerate(risk_sets.items()):\n",
    "    if index >= 3:  # Printing Only 3 Treated Patients and their Eligible Controls\n",
    "        break\n",
    "    print(f\"Treated Patient: {patient}\")\n",
    "    print(\"Eligible Control Patients:\")\n",
    "    print(controls)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52feb47-ee89-4523-b473-0f1dc16bbbeb",
   "metadata": {},
   "source": [
    "## III. Performing Optimal Matching using Minimum Cost Flow in a Network\n",
    "\n",
    "In this section, we describe the optimal matching process using minimum cost flow algorithms, which allow for efficient pairing of treated and control patients based on their covariate profiles.\n",
    "\n",
    "**Overview of Minimum Cost Flow** <br>\n",
    "The minimum cost flow algorithm is a network-based optimization method that seeks to minimize the total cost associated with transporting goods through a network while satisfying supply and demand constraints. In the context of patient matching, \"goods\" represent the matched pairs of treated and control patients, and \"cost\" corresponds to the distance (or dissimilarity) between their covariate profiles.\n",
    "\n",
    "**Constraints** <br>\n",
    "1. Each treated patient must be matched with exactly one control patient\n",
    "2. Each control patient can be matched with at most one treated patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d45be-b63f-48de-b1d6-afa91181fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def compute_cov_matrix(df, features):\n",
    "    cov_matrix = np.cov(df[features].dropna().values.T)\n",
    "    return np.linalg.inv(cov_matrix)  # Return inverse covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73050c08-31c2-4488-b20a-b41ba9533f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_cost_flow_matching(risk_sets, df, features=['Baseline Pain', 'Baseline Urgency', 'Baseline Frequency']):\n",
    "    \"\"\" Solves the optimal matching using minimum cost flow in a network \"\"\"\n",
    "    min_cost_flow_net = min_cost_flow.SimpleMinCostFlow()\n",
    "\n",
    "    # Node mapping and control usage tracking\n",
    "    treated_nodes = list(risk_sets.keys())\n",
    "    control_nodes = []\n",
    "    control_usage = {}\n",
    "\n",
    "    # Build control nodes list and count risk set appearances\n",
    "    for controls in risk_sets.values():\n",
    "        for _, control in controls.iterrows():\n",
    "            control_id = control['Patient ID']\n",
    "            if control_id not in control_usage:\n",
    "                control_nodes.append(control_id)\n",
    "                control_usage[control_id] = 0\n",
    "            control_usage[control_id] += 1\n",
    "\n",
    "    print(f\"🔹 Treated: {len(treated_nodes)}, Controls: {len(control_nodes)}\")\n",
    "\n",
    "    # Create node index mapping\n",
    "    node_index_map = {pid: i for i, pid in enumerate(treated_nodes + control_nodes)}\n",
    "    source = len(node_index_map)\n",
    "    sink = source + 1\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    cov_inv = compute_cov_matrix(df, features)\n",
    "\n",
    "    # Add treated-control edges with costs\n",
    "    for treated_id, controls in risk_sets.items():\n",
    "        treated_idx = node_index_map[treated_id]\n",
    "        treated_data = df[df['Patient ID'] == treated_id][features].values[0]\n",
    "        \n",
    "        for _, control in controls.iterrows():\n",
    "            control_id = control['Patient ID']\n",
    "            control_idx = node_index_map[control_id]\n",
    "            control_data = control[features].values\n",
    "            \n",
    "            distance = mahalanobis(treated_data, control_data, cov_inv)\n",
    "            if np.isnan(distance):\n",
    "                continue\n",
    "                \n",
    "            # Add edge with capacity 1 and scaled cost\n",
    "            min_cost_flow_net.add_arc_with_capacity_and_unit_cost(\n",
    "                treated_idx, control_idx, 1, int(distance * 100)\n",
    "            )\n",
    "\n",
    "    # Connect source to treated nodes (1 unit each)\n",
    "    for treated_id in treated_nodes:\n",
    "        min_cost_flow_net.add_arc_with_capacity_and_unit_cost(\n",
    "            source, node_index_map[treated_id], 1, 0\n",
    "        )\n",
    "\n",
    "    # Connect control nodes to sink with MULTIPLE USAGE capacity\n",
    "    for control_id in control_nodes:\n",
    "        capacity = control_usage[control_id]\n",
    "        min_cost_flow_net.add_arc_with_capacity_and_unit_cost(\n",
    "            node_index_map[control_id], sink, capacity, 0\n",
    "        )\n",
    "\n",
    "    # Set supply/demand (match all treated patients)\n",
    "    min_cost_flow_net.set_node_supply(source, len(treated_nodes))\n",
    "    min_cost_flow_net.set_node_supply(sink, -len(treated_nodes))\n",
    "\n",
    "    # Solve and extract matches\n",
    "    if min_cost_flow_net.solve() == min_cost_flow_net.OPTIMAL:\n",
    "        matches = {}\n",
    "        matched_controls = set()  # Set to track matched controls\n",
    "        \n",
    "        for i in range(min_cost_flow_net.num_arcs()):\n",
    "            if min_cost_flow_net.flow(i) > 0:\n",
    "                start = min_cost_flow_net.tail(i)\n",
    "                end = min_cost_flow_net.head(i)\n",
    "                \n",
    "                if start < len(treated_nodes) and end >= len(treated_nodes):\n",
    "                    treated_pid = treated_nodes[start]\n",
    "                    control_pid = control_nodes[end - len(treated_nodes)]\n",
    "                    \n",
    "                    # Check if this control patient has already been matched\n",
    "                    if control_pid not in matched_controls:\n",
    "                        matches[treated_pid] = control_pid\n",
    "                        matched_controls.add(control_pid)  # Mark this control as matched\n",
    "                    \n",
    "        print(f\"✅ Successfully matched {len(matches)} pairs\")\n",
    "        return matches\n",
    "    else:\n",
    "        print(\"❌ No solution: Possible reasons -\")\n",
    "        print(\"- Not enough eligible controls for all treated patients\")\n",
    "        print(\"- Network constraints too restrictive\")\n",
    "        print(\"- Extreme distance values causing cost overflow\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab251cdc-9a01-4c95-8830-33e6c9eb45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Risk Set Matching\n",
    "risk_sets = construct_risk_set(df)\n",
    "\n",
    "# Perform Optimal Matching\n",
    "matches = minimum_cost_flow_matching(risk_sets, df)\n",
    "\n",
    "print(\"Number of Edges: \", len(matches))\n",
    "\n",
    "# Print results\n",
    "# for treated, control in matches.items():\n",
    "#     print(f\"Treated Patient: {treated} → Matched Control: {control}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eda503-8c2e-4dae-a9eb-c964815ba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate matched control patients\n",
    "matched_controls = [control for _, control in matches.items()]\n",
    "if len(matched_controls) == len(set(matched_controls)):\n",
    "    print(\"All matches are unique!\")\n",
    "else:\n",
    "    print(\"Duplicate matches found! duplicates: \", len(matched_controls) - len(set(matched_controls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac07935-4a8d-4925-a5b9-1f1ceddbf3ca",
   "metadata": {},
   "source": [
    "## IV. Balanced Pair Matching Using Integer Programming (IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9ff75-756f-4dd3-81bc-d47d045bdf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.linear_solver import pywraplp\n",
    "\n",
    "def balanced_pair_matching_ip(df, covariate_pairs, quantiles=[0.33, 0.66], penalty=1e6):\n",
    "    # Binarize covariates (using pooled quantiles)\n",
    "    binary_features = []\n",
    "    for baseline_col, treatment_col in covariate_pairs:\n",
    "        # Use pooled data for quantiles\n",
    "        treated_data = df[df['Treatment Status'] == 1][baseline_col]\n",
    "        control_data = df[df['Treatment Status'] == 0][baseline_col]\n",
    "        pooled_data = pd.concat([treated_data, control_data])\n",
    "        q1 = pooled_data.quantile(quantiles[0])\n",
    "        q2 = pooled_data.quantile(quantiles[1])\n",
    "        \n",
    "        for col in [baseline_col, treatment_col]:\n",
    "            df[f'{col}_leq_q1'] = (df[col] <= q1).astype(int)\n",
    "            df[f'{col}_leq_q2'] = (df[col] <= q2).astype(int)\n",
    "            binary_features.extend([f'{col}_leq_q1', f'{col}_leq_q2'])\n",
    "    \n",
    "    # Check control count\n",
    "    treated = df[df['Treatment Status'] == 1].reset_index(drop=True)\n",
    "    controls = df[df['Treatment Status'] == 0].reset_index(drop=True)\n",
    "    if len(controls) < len(treated):\n",
    "        raise ValueError(\"Insufficient controls for matching.\")\n",
    "    \n",
    "    # Integer programming setup\n",
    "    solver = pywraplp.Solver.CreateSolver('SCIP')\n",
    "    pairs = [(t, c) for t in treated.index for c in controls.index]\n",
    "    x = {(t, c): solver.BoolVar(f'x_{t}_{c}') for t, c in pairs}\n",
    "    \n",
    "    # Matching constraints\n",
    "    for t in treated.index:\n",
    "        solver.Add(solver.Sum(x[(t, c)] for c in controls.index) == 1)\n",
    "    for c in controls.index:\n",
    "        solver.Add(solver.Sum(x[(t, c)] for t in treated.index) <= 1)\n",
    "    \n",
    "    # Balance constraints with penalties\n",
    "    objective = solver.Objective()\n",
    "    for feature in binary_features:\n",
    "        treated_total = solver.Sum(x[(t, c)] * treated.loc[t, feature] for t, c in pairs)\n",
    "        control_total = solver.Sum(x[(t, c)] * controls.loc[c, feature] for t, c in pairs)\n",
    "        imbalance = treated_total - control_total\n",
    "        solver.Add(imbalance <= penalty)\n",
    "        solver.Add(imbalance >= -penalty)\n",
    "        objective.SetCoefficient(imbalance, 1)  # Penalize absolute imbalance\n",
    "    \n",
    "    # Mahalanobis distance\n",
    "    cov_features = [col for pair in covariate_pairs for col in pair]\n",
    "    df[cov_features] = df[cov_features].fillna(df[cov_features].median())  # Impute missing\n",
    "    cov_matrix = np.cov(df[cov_features].values.T)\n",
    "    cov_inv = np.linalg.inv(cov_matrix)\n",
    "    \n",
    "    for (t, c) in pairs:\n",
    "        delta = mahalanobis(\n",
    "            treated.loc[t, cov_features].values,\n",
    "            controls.loc[c, cov_features].values,\n",
    "            cov_inv\n",
    "        )\n",
    "        objective.SetCoefficient(x[(t, c)], delta)\n",
    "    objective.SetMinimization()\n",
    "    \n",
    "    # Solve\n",
    "    status = solver.Solve()\n",
    "    if status == pywraplp.Solver.OPTIMAL:\n",
    "        return [(treated.loc[t, 'Patient ID'], controls.loc[c, 'Patient ID']) \n",
    "                for (t, c) in pairs if x[(t, c)].solution_value() > 0.5]\n",
    "    else:\n",
    "        print(\"No solution. Try relaxing constraints or checking data.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7bfbc-5f74-4d39-8686-514c49165b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define covariate pairs to binarize and balance\n",
    "covariate_pairs = [\n",
    "    ('Baseline Pain', 'Treatment Pain'),\n",
    "    ('Baseline Urgency', 'Treatment Urgency'),\n",
    "    ('Baseline Frequency', 'Treatment Frequency')\n",
    "]\n",
    "\n",
    "# Call the integrated function\n",
    "balanced_matches = balanced_pair_matching_ip(\n",
    "    df, \n",
    "    covariate_pairs, \n",
    "    quantiles=[0.33, 0.66]  # Tertiles\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for treated_id, control_id in balanced_matches:\n",
    "    print(f\"Treated Patient: {treated_id} → Balanced Matched Control: {control_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6816b13-ac65-4340-95cf-b1fecfba5c02",
   "metadata": {},
   "source": [
    "## V. Graphical Comparisons (IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96587fa-0895-4e5e-af3f-78e96f979fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Prepare the data for box plotsf\n",
    "# Assume that Treatment Time is in months and we have hypothetical values for post-treatment scores\n",
    "df['Treatment Pain (3 Months)'] = df['Treatment Pain'] - np.random.randint(0, 3, size=len(df)) # Simulating some change\n",
    "df['Treatment Pain (6 Months)'] = df['Treatment Pain'] - np.random.randint(0, 5, size=len(df)) # Simulating some change\n",
    "\n",
    "# Step 2: Calculate differences\n",
    "df['Difference Pain (3 Months)'] = df['Treatment Pain (3 Months)'] - df['Baseline Pain']\n",
    "df['Difference Pain (6 Months)'] = df['Treatment Pain (6 Months)'] - df['Baseline Pain']\n",
    "\n",
    "# Create a function to plot box plots for each covariate category\n",
    "def plot_boxplots(df):\n",
    "    categories = {\n",
    "        \"Baseline\": ['Baseline Pain', 'Baseline Urgency', 'Baseline Frequency'],\n",
    "        \"At Treatment\": ['Treatment Pain', 'Treatment Urgency', 'Treatment Frequency'],\n",
    "        \"3 Months Post-Treatment\": ['Treatment Pain (3 Months)', 'Treatment Urgency', 'Treatment Frequency'],\n",
    "        \"6 Months Post-Treatment\": ['Treatment Pain (6 Months)', 'Treatment Urgency', 'Treatment Frequency'],\n",
    "        \"Difference (3 Months)\": ['Difference Pain (3 Months)'],\n",
    "        \"Difference (6 Months)\": ['Difference Pain (6 Months)']\n",
    "    }\n",
    "\n",
    "    for title, vars in categories.items():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Melt the DataFrame for the current category variables\n",
    "        melted_df = pd.melt(df[df['Treatment Status'].isin([0, 1])], \n",
    "                             id_vars=['Patient ID', 'Treatment Status'], \n",
    "                             value_vars=vars,\n",
    "                             var_name='Covariate',\n",
    "                             value_name='Score')\n",
    "\n",
    "        # Create a box plot for the current category\n",
    "        sns.boxplot(x='Covariate', y='Score', hue='Treatment Status', data=melted_df)\n",
    "        plt.title(f'Box Plots of {title}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Covariate')\n",
    "        plt.legend(title='Group', labels=['Never Treated', 'Treated'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Call the function to create box plots\n",
    "plot_boxplots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4054f4-974a-4ff3-831d-ad3492ed2878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
